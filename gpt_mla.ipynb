{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff9c19774d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "import math\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "\n",
    "wandb.init(project=\"gpt_mla\", name='v2')\n",
    "\n",
    "torch.manual_seed(1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前置代码\n",
    "class DeepseekV2RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "\n",
    "# 位置编码\n",
    "class DeepseekV2RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (\n",
    "            self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim)\n",
    "        )\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        # 较小索引位置对应较低频率\n",
    "        # 较大的索引位置有较高的频率\n",
    "        \n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings,\n",
    "            device=self.inv_freq.device,\n",
    "            dtype=torch.get_default_dtype(),\n",
    "        )\n",
    "        self.max_seq_len_cached = None\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(\n",
    "            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype\n",
    "        )\n",
    "\n",
    "        freqs = torch.outer(t, self.inv_freq.to(t.device))\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if self.max_seq_len_cached is None or seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "\n",
    "# Copied from transformers.models.llama.modeling_llama.rotate_half\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    b, h, s, d = q.shape\n",
    "    q = q.view(b, h, s, d // 2, 2).transpose(4, 3).reshape(b, h, s, d)\n",
    "\n",
    "    b, h, s, d = k.shape\n",
    "    k = k.view(b, h, s, d // 2, 2).transpose(4, 3).reshape(b, h, s, d)\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb_v2(q: torch.Tensor, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    b, h, s, d = q.shape\n",
    "    q = q.view(b, h, s, d // 2, 2).transpose(4, 3).reshape(b, h, s, d)\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    return q_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 定义 GPT 相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 512 # seq_len\n",
    "    batch_size: int = 12    \n",
    "    n_layer: int = 6\n",
    "    n_head: int = 8\n",
    "    n_embed: int = 2048\n",
    "    # head_size: int = 128\n",
    "    dropout: float = 0.1 \n",
    "    vocab_size: int = 50257\n",
    "    use_cache: bool = False\n",
    "\n",
    "\n",
    "    hidden_size: int = 2048\n",
    "    num_heads: int = 8\n",
    "    max_position_embeddings: int = 512\n",
    "    rope_theta: float = 128000\n",
    "    attention_dropout: float = 0.1\n",
    "    q_lora_rank: int = 256 # 得到低维度q的低秩变换矩阵\n",
    "    qk_rope_head_dim: int = 64 # qv向量所需 rope 位置编码向量的维度\n",
    "    kv_lora_rank: int = 64 # 得到低维度kv的低秩变换矩阵 \n",
    "    v_head_dim: int  = 128 # v向量升秩后的维度\n",
    "    qk_nope_head_dim: int = 128  # qk向量升秩后的维度  \n",
    "    attention_bias: bool = False\n",
    "    \n",
    "    training: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DeepseekConfig:\n",
    "    hidden_size: int\n",
    "    num_heads: int\n",
    "    max_position_embeddings: int\n",
    "    rope_theta: float\n",
    "\n",
    "    attention_dropout: float\n",
    "\n",
    "    q_lora_rank: int # 得到低维度q的低秩变换矩阵\n",
    "    qk_rope_head_dim: int # qv向量所需 rope 位置编码向量的维度\n",
    "\n",
    "    kv_lora_rank: int # 得到低维度kv的低秩变换矩阵 \n",
    "\n",
    "    v_head_dim: int # v向量升秩后的维度\n",
    "    qk_nope_head_dim: int # qk向量升秩后的维度  \n",
    "    attention_bias: bool \n",
    "    \n",
    "    training: bool = True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SingleHeadAttention(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super(SingleHeadAttention, self).__init__()\n",
    "#         self.key = nn.Linear(config.n_embed, config.head_size)\n",
    "#         self.query = nn.Linear(config.n_embed, config.head_size)\n",
    "#         self.value = nn.Linear(config.n_embed, config.head_size)\n",
    "\n",
    "#         self.head_size = config.head_size\n",
    "#         self.use_cache = config.use_cache\n",
    "\n",
    "#         # 不计算梯度的方式\n",
    "#         self.register_buffer(\n",
    "#             \"attention_mask\",\n",
    "#             torch.tril(\n",
    "#                 torch.ones(\n",
    "#                     config.block_size, config.block_size\n",
    "#                 )\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "#         # 缓存k,v\n",
    "#         self.k_cache = None\n",
    "#         self.v_cache = None\n",
    "#         self.out_cache = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size, seq_len, embed_size = x.size()\n",
    "\n",
    "#         # 使用缓存\n",
    "#         if self.use_cache:\n",
    "            \n",
    "#             q_to_use = None\n",
    "\n",
    "#             if self.k_cache is None:\n",
    "#                 self.k_cache = self.key(x)\n",
    "#                 self.v_cache = self.value(x)\n",
    "#                 q_to_use = self.query(x)\n",
    "#             else:\n",
    "#                 # 添加k,v新的行\n",
    "#                 k = self.key(x[:, -1, :]).unsqueeze(1)\n",
    "#                 v = self.value(x[:, -1, :]).unsqueeze(1)\n",
    "\n",
    "#                 self.k_cache = torch.cat([self.k_cache, k], dim=1)\n",
    "#                 self.v_cache = torch.cat([self.v_cache, v], dim=1)\n",
    "            \n",
    "#                 q_to_use = self.query(x[:, -1, :]).unsqueeze(1)\n",
    "\n",
    "\n",
    "#             weight = q_to_use @ self.k_cache.transpose(-2, -1) # [batch_size, 1 or init, h_size] @ [batch_size, h_size, seq_len]\n",
    "\n",
    "#             weight = F.softmax(weight / (self.head_size ** 0.5), dim=-1) \n",
    "#             weight = self.dropout(weight)\n",
    "#             new_token = weight @ self.v_cache\n",
    "\n",
    "            \n",
    "#             if self.out_cache is not None:\n",
    "#                 # print(\"拼接outcache\")\n",
    "#                 self.out_cache = torch.cat([self.out_cache, new_token], dim=1)\n",
    "#                 # print(f\"self.out_cahe: {self.out_cache.shape}\")\n",
    "#             else:\n",
    "#                 self.out_cache = new_token\n",
    "\n",
    "#             return self.out_cache   \n",
    "            \n",
    "\n",
    "\n",
    "#         else:\n",
    "#             k, q, v = self.key(x), self.query(x), self.value(x)\n",
    "\n",
    "#             weight = q @ k.transpose(-2, -1)\n",
    "            \n",
    "#             # mask\n",
    "#             weight = weight.masked_fill(\n",
    "#                 self.attention_mask[:seq_len, :seq_len] == 0,\n",
    "#                 float(\"-inf\")\n",
    "#             )\n",
    "            \n",
    "#             weight = F.softmax(weight / (self.head_size ** 0.5), dim=-1)\n",
    "#             weight = self.dropout(weight)\n",
    "#             out = weight @ v\n",
    "\n",
    "#             return out\n",
    "\n",
    "class MLA(nn.Module):\n",
    "    '''包含矩阵吸收'''\n",
    "    def __init__(self, config: DeepseekConfig):\n",
    "        super(MLA, self).__init__()\n",
    "        self.training = config.training\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.v_head_dim = config.v_head_dim\n",
    "        self.qk_nope_head_dim = config.qk_nope_head_dim\n",
    "        self.qk_rope_head_dim = config.qk_rope_head_dim\n",
    "\n",
    "\n",
    "        self.out_proj = nn.Linear(\n",
    "            self.num_heads * self.v_head_dim,\n",
    "            self.hidden_size,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # 压缩\n",
    "        self.q_lora_rank = config.q_lora_rank  \n",
    "        self.kv_lora_rank = config.kv_lora_rank\n",
    "\n",
    "        \n",
    "        self.q_down_proj = nn.Linear(\n",
    "            self.hidden_size,\n",
    "            self.q_lora_rank,\n",
    "            bias=config.attention_bias\n",
    "        )\n",
    "        \n",
    "        self.q_down_norm = DeepseekV2RMSNorm(self.q_lora_rank)\n",
    "\n",
    "        # k在降维时得到位置编码\n",
    "        self.kv_down_proj = nn.Linear(\n",
    "            self.hidden_size,\n",
    "            self.kv_lora_rank + config.qk_rope_head_dim,\n",
    "            bias=config.attention_bias\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "        self.kv_down_norm = DeepseekV2RMSNorm(self.kv_lora_rank + self.qk_rope_head_dim)\n",
    "\n",
    "        # 升维\n",
    "        self.q_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim # \n",
    "\n",
    "        # q向量在升维时得到位置编码\n",
    "        self.q_up_proj = nn.Linear(\n",
    "            self.q_lora_rank,\n",
    "            self.num_heads * self.q_head_dim,\n",
    "            bias= False, # \n",
    "        )\n",
    "\n",
    "        # qk升维用的是同一个低秩向量，这里同时进行kv的升维，所以映射后的维度要包括q和v的\n",
    "        self.kv_up_proj = nn.Linear(\n",
    "            self.kv_lora_rank,\n",
    "            self.num_heads * (config.qk_nope_head_dim + config.v_head_dim),\n",
    "            bias=False\n",
    "        ) \n",
    "\n",
    "        self.rotary_emb = DeepseekV2RotaryEmbedding(\n",
    "            config.qk_rope_head_dim,\n",
    "            config.max_position_embeddings,\n",
    "            config.rope_theta\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, position_ids, attention_mask=None):\n",
    "        b, s, d = hidden_states.shape\n",
    "        # q part\n",
    "        q = self.q_down_proj(hidden_states)\n",
    "        q = self.q_down_norm(q)\n",
    "        q = self.q_up_proj(q) # num_heads * ( nope_dim + rope_dim )\n",
    "\n",
    "        # [b, s, num_heads * (nope+rope)] -> [b, num_heads, s, nope+rope]\n",
    "        q = q.view(b, s, self.num_heads, self.q_head_dim).transpose(1, 2)\n",
    "        # split q to q_nope and q_rope\n",
    "        q_nope, q_rope = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "        \n",
    "        # k v part\n",
    "        c_kv_and_rope = self.kv_down_proj(hidden_states)\n",
    "        c_kv_and_rope = self.kv_down_norm(c_kv_and_rope)\n",
    "        c_kv, k_rope = c_kv_and_rope.split([self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n",
    "\n",
    "        # [b, s, 1, rope_dim] -> [b, 1, s, rope_dim]\n",
    "        k_rope = k_rope.view(b, s, 1, self.qk_rope_head_dim).transpose(1, 2)\n",
    "\n",
    "        # 从 kv_up_proj 中分离出 W_UK 和 W_UV\n",
    "        kv_b_proj = self.kv_up_proj.weight.view(\n",
    "            self.num_heads, -1, self.kv_lora_rank\n",
    "        )\n",
    "\n",
    "        q_absorb = kv_b_proj[:, :self.qk_nope_head_dim, :]\n",
    "        out_absorb = kv_b_proj[:, self.qk_nope_head_dim:, :]\n",
    "\n",
    "\n",
    "        cos, sin = self.rotary_emb(q_rope, seq_len=s)\n",
    "        q_rope = apply_rotary_pos_emb_v2(\n",
    "            q_rope, cos, sin, position_ids\n",
    "        )\n",
    "\n",
    "\n",
    "        # W_UK被q_nope吸收\n",
    "        q_nope = torch.einsum('hdc, bhqd->bhqc', q_absorb, q_nope)\n",
    "\n",
    "\n",
    "        attn_weights = torch.matmul(q_rope, k_rope.transpose(-1, -2)) + torch.einsum('bhqc, blc->bhql', q_nope, c_kv)\n",
    "        attn_weights = attn_weights / math.sqrt(self.q_head_dim)\n",
    "\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attn_weights = torch.masked_fill(\n",
    "                attn_weights,\n",
    "                attention_mask == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        \n",
    "        attn_weights = F.softmax(attn_weights, dim=-1).to(hidden_states.dtype)\n",
    "\n",
    "        attn_weights = F.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "\n",
    "        o_  = torch.einsum('bhql,blc->bhqc', attn_weights, c_kv) # (4)\n",
    "        o   = torch.einsum('bhqc,hdc->bhqd', o_, out_absorb)  # (5)\n",
    "        u   = torch.einsum('hdD,bhqd->bqD', self.out_proj.weight.view(self.num_heads, self.v_head_dim, -1), o)     # (6)\n",
    "\n",
    "        return u, attn_weights\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.mla = MLA(config)\n",
    "\n",
    "        self.fc = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.mla(x,torch.arange(\n",
    "                self.max_position_embeddings,\n",
    "                ).unsqueeze(0).expand(\n",
    "                    x.size(0), -1\n",
    "                ) \n",
    "        )\n",
    "\n",
    "        output = self.fc(output)\n",
    "        output = self.dropout(output)\n",
    "        # print(f\"multi ouput: {output.shape}\")        \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embed, 4 * config.n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embed, config.n_embed),\n",
    "            nn.Dropout(config.dropout)\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.att = MultiHeadAttention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.ln1 = nn.LayerNorm(config.n_embed)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意残差链接 GPT模型使用的是前归一化\n",
    "\n",
    "        # print(f'x: {x.shape}, att: {self.att(self.ln1(x)).shape}')\n",
    "\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        \n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 完整的GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT, self).__init__()\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embed)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                Block(config) for _ in range(config.n_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(config.n_embed)\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.02, mean=0)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, std=0.02, mean=0)\n",
    "\n",
    "\n",
    "    def forward(self, ids, targets=None):\n",
    "        batch_size, seq_len = ids.size()\n",
    "        token_embedding = self.token_embedding_table(ids)\n",
    "        # position_ids = torch.arange(seq_len, device=ids.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        position_ids = torch.arange(seq_len, device=ids.device)\n",
    "\n",
    "        position_embedding = self.position_embedding_table(position_ids)\n",
    "\n",
    "        x = token_embedding + position_embedding # 广播机制\n",
    "\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            _, seq_len, vocab_size = logits.size()\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'CausalLM/Refined-Anime-Text'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# # 对数据集加载并排序\n",
    "# ds = load_dataset(dataset_name)\n",
    "# ds_add_len = ds['train'].map(lambda x : {\"length\" : len(x['text']) }, num_proc=8)\n",
    "# sorted_ds = ds_add_len.sort('length')\n",
    "# # 取前半数据\n",
    "# dataset = sorted_ds[: len(sorted_ds) // 2 ]\n",
    "# dataset = Dataset.from_dict(dataset)\n",
    "# dataset = dataset.remove_columns(['length'])\n",
    "# tokenized_data = dataset.map(\n",
    "#     lambda x: tokenizer(x['text'], return_tensors='pt', padding=True, truncation=True, max_length=2048),\n",
    "#     batched=True,\n",
    "#     remove_columns=['text'],\n",
    "#     num_proc=8\n",
    "# )\n",
    "# selected_data = tokenized_data.shuffle().select(range(10))\n",
    "# for data in selected_data:\n",
    "#     print(len(data['input_ids']))\n",
    "# block_size = 512\n",
    "# def group_texts(examples):\n",
    "#     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "#     total_length = len(concatenated_examples[\"input_ids\"])\n",
    "#     total_length = (total_length // (block_size + 1)) * (block_size + 1)\n",
    "#     result = {\n",
    "#         k: [t[i : i + block_size + 1] for i in range(0, total_length, block_size + 1)]\n",
    "#         for k, t in concatenated_examples.items()\n",
    "#     }\n",
    "#     result[\"labels\"] = [seq[1:] for seq in result[\"input_ids\"]]\n",
    "#     result[\"input_ids\"] = [seq[:-1] for seq in result[\"input_ids\"]]\n",
    "#     return result\n",
    "\n",
    "# chunked_data = tokenized_data.map(group_texts, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked_data.save_to_disk('./dataset/chunked_data_seq512')\n",
    "chunked_data = load_from_disk('./dataset/chunked_data_seq512/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  464,   464,   464,   464,   162,   253,   232, 42468, 25465,   164,\n",
       "          251,   236, 41753,   100, 16764,   164,   232,   247,   164,   243,\n",
       "          122, 42468, 20804, 33768,   237, 16764, 30585,   243,   162,   107,\n",
       "          242, 42468, 20804, 33768,   237, 16764,   165,   247,   228,   162,\n",
       "          110,   231, 42468, 26193,   222, 33768,   237, 16764,   162,    96,\n",
       "          106,   162,   122,   250, 42468, 43718,   239,   162,   229,   240,\n",
       "        16764, 20046,   242,   164,   233,   241, 42468, 20998,   234,   165,\n",
       "          109,   120, 41753,   100, 16764,   164,   236,   104, 36685,   106,\n",
       "        39355,    94, 42468, 20804, 33768,   237, 16764,   164,   236,   231,\n",
       "          164,   236,   231, 42468,   163,   251,    94, 20804, 33768,   237,\n",
       "        16764, 27670,   232,   164,   236,   231, 12859,   248, 42468, 20804,\n",
       "        33768,   237, 16764, 24376, 43145, 10310,   119,   164,   100,   240,\n",
       "        42468,   164,   233,   237,   162,   248,   244,   162,   248,   244,\n",
       "        16764, 24376, 43145, 46763,   244,   163,   119,   104, 42468, 34932,\n",
       "          239, 31965,   249, 41753,   100, 16764, 39355,   223,   164,   244,\n",
       "          108, 42468, 32432,   101,   164,   253,   117, 41753,   100, 16764,\n",
       "        25001,   230, 49426,   228, 42468, 12859,   248, 21689,   163,   109,\n",
       "          119, 16764, 30585,   228,   163,   101,   222, 42468, 11737,   247,\n",
       "        21689, 33768,   237, 16764, 36365,   112, 12859,   239, 42468, 31660,\n",
       "        20998,   103,   163,   234,   104, 16764, 24376, 43145, 39355,   223,\n",
       "          164,   244,   108, 42468, 32432,   101,   164,   253,   117, 41753,\n",
       "          100, 16764, 24376, 43145,   163,   241,   250, 36310, 42468, 34932,\n",
       "          239, 31965,   249, 41753,   100, 16764,   161,   109,   109, 33176,\n",
       "          116, 37605,    99, 42468,   163,   234,   236, 21689, 16764,   162,\n",
       "          253,   232, 30266,   237, 25001,   230, 42468, 20804, 33768,   237,\n",
       "        16764, 24376, 43145, 24376, 43145, 24376, 43145, 24376, 43145, 34932,\n",
       "          239, 21410, 34650,   238, 34650,   238, 42468,   163,   100,   233,\n",
       "        16764, 20998,   114,   164,   122,   231, 42468, 25465,   163,   100,\n",
       "           97, 41753,   100, 16764, 24376, 43145, 36181,   115, 20998,    97,\n",
       "          162,   233,   231, 42468, 36685,   244, 33768,   237, 16764,   164,\n",
       "        46788, 39355,   115, 42468, 45379,   106, 36310, 41753,   100, 16764,\n",
       "        36365,   112,   165,    98,   118, 42468,   163, 50159,   163,   122,\n",
       "          232, 41753,   100, 16764, 24376, 43145,   164,   236,   104, 36685,\n",
       "          106, 39355,    94, 42468, 20804, 33768,   237, 16764, 24376, 43145,\n",
       "         5308, 29119, 37239,   101,   163,   229,   243, 24376, 43145, 24376,\n",
       "        43145, 24376, 43145,   164,   236,   104, 36685,   106, 39355,    94,\n",
       "        42468, 20804, 33768,   237, 16764, 20046,   251, 39355,   236, 42468,\n",
       "        25465,   164,   251,   236, 41753,   100, 16764,   164,   236,   104,\n",
       "        36685,   106, 39355,    94, 42468, 20804, 33768,   237, 16764,   164,\n",
       "          231,   122,   164,   236,   110, 42468, 45379,   106, 36310, 33768,\n",
       "          237, 16764, 39355,   223,   164,   244,   108, 42468, 32432,   101,\n",
       "          164,   253,   117, 41753,   100, 16764,   165,   246,   123, 47947,\n",
       "          240, 13783,   104, 42468, 22755,   246, 18803, 16764, 24376, 43145,\n",
       "         5308, 29119, 16268,   249,   101,   165,   241,   225, 17739,   108,\n",
       "        42468, 20804,   161,   109,   252, 45250,   100, 16764,   164,   231,\n",
       "          122,   164,   236,   110, 42468, 45379,   106, 36310, 33768,   237,\n",
       "        16764, 37345,   254,   163,   237,   252, 42468,  1433,   161,   110,\n",
       "          223, 16764, 12859,   248, 20998,   114, 42468,   164,   237,   110,\n",
       "          162,   252,   245, 33768,   237, 16764, 38184,   115, 22887,   242,\n",
       "        43718,   120, 42468,   165,   103, 39374, 16764, 39355,    94,   164,\n",
       "          240,   224, 42468, 11737,   247, 21689, 33768,   237, 16764, 24376,\n",
       "        43145, 24376])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_data[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(chunked_data, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56567"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 453.970176M\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f'Total parameters: {total_params/ 1e6}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "sheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, sheduler, train_loader, test_loader, device, epoch):\n",
    "    model.train()\n",
    "  \n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        ids = batch['input_ids'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "\n",
    "        # 计算loss\n",
    "        logits, loss = model(ids, targets)\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 调整学习率\n",
    "        sheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 打印信息\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f' Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "\n",
    "            checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'sheduler_state_dict': sheduler.state_dict(),\n",
    "            'train_loss': loss,\n",
    "        }\n",
    "        \n",
    "        if batch_idx % 10000 == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "            # 定义保存路径\n",
    "            save_path = f'checkpoint/gpt_all/epoch{epoch}-{batch_idx}.pth'\n",
    "\n",
    "            # 检查文件夹是否存在，如果不存在则创建\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "            # 保存模型\n",
    "            torch.save(checkpoint, save_path)   \n",
    "\n",
    "\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, test_loader, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            targets = batch['labels'].to(device)\n",
    "\n",
    "            logits, loss = model(ids, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            print(f'Val Loss: {val_loss / len(test_loader):.4f}')\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    return val_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt(epoch_num = 3, ):\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "\n",
    "        train_loss = train(model, optimizer, sheduler, train_dataloader, None, device, epoch)\n",
    "        # val_loss = eval(model, test_loader, device)\n",
    "\n",
    "        print(f'Epoch: {epoch}, Train Loss: {train_loss}')\n",
    "\n",
    "\n",
    "    \n",
    "    print('finish training!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch: 0, Loss: 11.3040\n",
      " Batch: 100, Loss: 3.8349\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 5\u001b[0m, in \u001b[0;36mtrain_gpt\u001b[0;34m(epoch_num)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_gpt\u001b[39m(epoch_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, ):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_num):\n\u001b[0;32m----> 5\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# val_loss = eval(model, test_loader, device)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, sheduler, train_loader, test_loader, device, epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 调整学习率\u001b[39;00m\n\u001b[1;32m     19\u001b[0m sheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 打印信息\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_gpt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于缺少模块 \"pygments\"，kernel 启动失败。请考虑安装此模块。\n",
      "\u001b[1;31m有关详细信息，请单击 <a href=\"https://aka.ms/kernelFailuresMissingModule\">此处</a>。"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于缺少模块 \"pygments\"，kernel 启动失败。请考虑安装此模块。\n",
      "\u001b[1;31m有关详细信息，请单击 <a href=\"https://aka.ms/kernelFailuresMissingModule\">此处</a>。"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m由于缺少模块 \"pygments\"，kernel 启动失败。请考虑安装此模块。\n",
      "\u001b[1;31m有关详细信息，请单击 <a href=\"https://aka.ms/kernelFailuresMissingModule\">此处</a>。"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "\n",
    "# # 进行推理\n",
    "# # dataset = MyDataset('./dataset/anime_text/')\n",
    "# chpt_path = './checkpoint/gpt_all/epoch0-100.pth'\n",
    "\n",
    "# infer_config = GPTConfig()\n",
    "# infer_config.use_cache = True\n",
    "# model = GPT(infer_config)\n",
    "# checkpoint = torch.load(chpt_path)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# no_cache_config = GPTConfig()\n",
    "# no_cache_config.use_cache = False\n",
    "# model_no_cache = GPT(no_cache_config)\n",
    "# checkpoint = torch.load(chpt_path)\n",
    "# model_no_cache.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# def inference(model, prompt, max_len):\n",
    "#     model.eval()\n",
    "#     model = model.to(device)\n",
    "#     ids = torch.tensor(dataset.encode(prompt)).to(device)\n",
    "#     ids = ids.unsqueeze(0)\n",
    "#     # print(ids)\n",
    "#     res = []\n",
    "#     for _ in range(max_len):\n",
    "#         b, sl = ids.size()\n",
    "#         if sl > GPTConfig.block_size:\n",
    "#             ids = ids[:, :-GPTConfig.block_size]\n",
    "       \n",
    "#         logits, _ = model(ids, targets=None)\n",
    "#         logits = logits[:, -1, :]\n",
    "#         probs = F.softmax(logits, dim = -1)\n",
    "#         idx_new = torch.multinomial(probs, num_samples=1)\n",
    "#         ids = torch.cat((ids, idx_new), dim=1)\n",
    "#     ids = ids[:, -max_len:].flatten()\n",
    "#     # print(ids)\n",
    "#     return dataset.decode(ids)\n",
    "\n",
    "# while 1:\n",
    "#     prompt = input(\"我:\")\n",
    "\n",
    "\n",
    "#     start = time.time()\n",
    "#     res = inference(model_no_cache, prompt, 300)\n",
    "#     end = time.time()\n",
    "#     print(f'无cache耗时： {end-start} \\n 我：{prompt} \\n chat-bot: {res}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
