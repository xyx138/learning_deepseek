{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3a1022c270>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "wandb.init(project=\"my_gpt2\", name='add_kv_cache')\n",
    "\n",
    "torch.manual_seed(1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 定义 GPT 相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 512\n",
    "    batch_size: int = 12    \n",
    "    n_layer: int = 6\n",
    "    n_head: int = 12\n",
    "    n_embed: int = 768\n",
    "    head_size: int = n_embed // n_head\n",
    "    dropout: float = 0.1 \n",
    "    vocab_size: int = 50257\n",
    "    use_cache: bool = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.key = nn.Linear(config.n_embed, config.head_size)\n",
    "        self.query = nn.Linear(config.n_embed, config.head_size)\n",
    "        self.value = nn.Linear(config.n_embed, config.head_size)\n",
    "\n",
    "        self.head_size = config.head_size\n",
    "        self.use_cache = config.use_cache\n",
    "\n",
    "        # 不计算梯度的方式\n",
    "        self.register_buffer(\n",
    "            \"attention_mask\",\n",
    "            torch.tril(\n",
    "                torch.ones(\n",
    "                    config.block_size, config.block_size\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # 缓存k,v\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "        self.out_cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_size = x.size()\n",
    "\n",
    "        # 使用缓存\n",
    "        if self.use_cache:\n",
    "            if self.k_cache is None:\n",
    "                self.k_cache = self.key(x)\n",
    "                self.v_cache = self.value(x)\n",
    "            else:\n",
    "                # 添加k,v新的行\n",
    "                k = self.key(x[:, -1, :])\n",
    "                v = self.value(x[:, -1, :])\n",
    "\n",
    "                self.k_cache = torch.cat([self.k_cache, k], dim=1)\n",
    "                self.v_cache = torch.cat([self.v_cache, v], dim=1)\n",
    "            \n",
    "            q = self.query(x[:, -1, :])\n",
    "\n",
    "            weight = q @ self.k_cache.transpose(-2, -1) # (batch_size, 1, head_size) @ (batch_size, head_size, seq_len) = (batch_size, 1, seq_len)\n",
    "\n",
    "            weight = F.softmax(weight / (self.head_size ** 0.5), dim=-1)\n",
    "            weight = self.dropout(weight)\n",
    "            new_token = weight @ self.v_cache # (batch_size, 1, seq_len) @ (batch_size, seq_len, head_size) = (batch_size, 1, head_size)\n",
    "\n",
    "            if self.out_cache:\n",
    "                self.out_cache = torch.cat([self.out_cache, new_token], dim=1)\n",
    "            else:\n",
    "                self.out_cache = new_token\n",
    "\n",
    "            return self.out_cache   \n",
    "            \n",
    "\n",
    "\n",
    "        else:\n",
    "            k, q, v = self.key(x), self.query(x), self.value(x)\n",
    "\n",
    "            weight = q @ k.transpose(-2, -1)\n",
    "            \n",
    "            # mask\n",
    "            weight = weight.masked_fill(\n",
    "                self.attention_mask[:seq_len, :seq_len] == 0,\n",
    "                float(\"-inf\")\n",
    "            )\n",
    "            \n",
    "            weight = F.softmax(weight / (self.head_size ** 0.5), dim=-1)\n",
    "            weight = self.dropout(weight)\n",
    "            out = weight @ v\n",
    "\n",
    "            return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                SingleHeadAttention(config) for _ in range(config.n_head)\n",
    "            ]\n",
    "        )\n",
    "        self.fc = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat(\n",
    "            [\n",
    "                head(x) for head in self.heads\n",
    "            ],\n",
    "            dim=-1\n",
    "        )\n",
    "        output = self.fc(output)\n",
    "        output = self.dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embed, 4 * config.n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embed, config.n_embed),\n",
    "            nn.Dropout(config.dropout)\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.att = MultiHeadAttention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.ln1 = nn.LayerNorm(config.n_embed)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意残差链接 GPT模型使用的是前归一化\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 完整的GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT, self).__init__()\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embed)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                Block(config) for _ in range(config.n_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.ln_final = nn.LayerNorm(config.n_embed)\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.02, mean=0)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, std=0.02, mean=0)\n",
    "\n",
    "\n",
    "    def forward(self, ids, targets=None):\n",
    "        batch_size, seq_len = ids.size()\n",
    "        token_embedding = self.token_embedding_table(ids)\n",
    "        # position_ids = torch.arange(seq_len, device=ids.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        position_ids = torch.arange(seq_len, device=ids.device)\n",
    "\n",
    "        position_embedding = self.position_embedding_table(position_ids)\n",
    "\n",
    "        x = token_embedding + position_embedding # 广播机制\n",
    "\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            _, seq_len, vocab_size = logits.size()\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__ (self, path, block_size=512):\n",
    "\n",
    "        import tiktoken\n",
    "\n",
    "        self.enc = tiktoken.get_encoding('gpt2')\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.eos_token = self.enc.encode(\n",
    "            '<|endoftext|>',allowed_special={\"<|endoftext|>\"})[0]\n",
    "        \n",
    "        import json\n",
    "\n",
    "        self.encoded_data = []\n",
    "\n",
    "        raw_data = load_from_disk(path)\n",
    "\n",
    "        eos_encoded = []\n",
    "        for text in raw_data['train']['text'][:]:\n",
    "\n",
    "            # print(example)\n",
    "            encoded_text = self.enc.encode(text)\n",
    "            eos_encoded.extend(encoded_text + [self.eos_token])\n",
    "        \n",
    "\n",
    "        self.exmaples = []\n",
    "\n",
    "        # 分割出训练样本\n",
    "        for i in range(0, len(eos_encoded), block_size):\n",
    "            chunk = eos_encoded[i:i+block_size + 1] \n",
    "            if len(chunk) < block_size + 1:\n",
    "                chunk += [self.eos_token] * (block_size + 1 - len(chunk))\n",
    "            self.exmaples.append(chunk)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.exmaples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.exmaples[idx]\n",
    "        x = torch.tensor(chunk[:-1])\n",
    "        y = torch.tensor(chunk[1:])\n",
    "        return {\n",
    "            \"input_ids\": x,\n",
    "            \"labels\": y\n",
    "        }\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"编码器\"\"\"\n",
    "        return self.enc.encode(text) \n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"解码器\"\"\"\n",
    "        ids = list(ids)\n",
    "        return self.enc.decode(ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset('./dataset/anime_text/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355791"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320212, 35579)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = DataLoader(train_dataset, batch_size=12, shuffle=True), DataLoader(test_dataset, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 120.116736M\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f'Total parameters: {total_params/ 1e6}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "sheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  115,   163,   230,  ...,   225, 12859,   228],\n",
       "         [  171,   120,   249,  ...,   290, 15347,  2095],\n",
       "         [49035,   119, 27950,  ...,   240,   164,   231],\n",
       "         ...,\n",
       "         [  236, 26344,   246,  ...,   171,   120,   234],\n",
       "         [21410, 31660, 37772,  ..., 13160,   286,   262],\n",
       "         [  238,   171,   120,  ...,    34, 25634, 20025]]),\n",
       " 'labels': tensor([[  163,   230,   115,  ..., 12859,   228, 31660],\n",
       "         [  120,   249, 25001,  ..., 15347,  2095,   287],\n",
       "         [  119, 27950,   249,  ...,   164,   231,   110],\n",
       "         ...,\n",
       "         [26344,   246, 13783,  ...,   120,   234, 26193],\n",
       "         [31660, 37772,   246,  ...,   286,   262, 43998],\n",
       "         [  171,   120,   231,  ..., 25634, 20025,   171]])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9717, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    ids = batch['input_ids'].to(device)\n",
    "    targets = batch['labels'].to(device)\n",
    "    logits, loss = model(ids, targets)\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26685"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, sheduler, train_loader, test_loader, device):\n",
    "    model.train()\n",
    "  \n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        ids = batch['input_ids'].to(device)\n",
    "        targets = batch['labels'].to(device)\n",
    "\n",
    "        # 计算loss\n",
    "        logits, loss = model(ids, targets)\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 调整学习率\n",
    "        sheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 打印信息\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f' Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, test_loader, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            targets = batch['labels'].to(device)\n",
    "\n",
    "            logits, loss = model(ids, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            print(f'Val Loss: {val_loss / len(test_loader):.4f}')\n",
    "\n",
    "    return val_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt(epoch_num = 3):\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "\n",
    "        train_loss = train(model, optimizer, sheduler, train_loader, test_loader, device)\n",
    "        val_loss = eval(model, test_loader, device)\n",
    "\n",
    "        print(f'Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'sheduler_state_dict': sheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "        # 定义保存路径\n",
    "        save_path = f'checkpoint/gpt_all/epoch_{epoch}.pth'\n",
    "\n",
    "        # 检查文件夹是否存在，如果不存在则创建\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "        # 保存模型\n",
    "        torch.save(checkpoint, save_path)   \n",
    "        \n",
    "\n",
    "\n",
    "        torch.save(checkpoint, save_path)\n",
    "    \n",
    "    print('finish training!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_gpt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38673/1505245239.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(chpt_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataset('./dataset/anime_text/')\n",
    "chpt_path = './checkpoint/gpt_all/epoch_0.pth'\n",
    "model = GPT(GPTConfig())\n",
    "checkpoint = torch.load(chpt_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行推理\n",
    "\n",
    "\n",
    "def inference(model, prompt, max_len):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    ids = torch.tensor(dataset.encode(prompt)).to(device)\n",
    "    ids = ids.unsqueeze(0)\n",
    "    # print(ids)\n",
    "    res = []\n",
    "    for _ in range(max_len):\n",
    "        b, sl = ids.size()\n",
    "        if sl > GPTConfig.block_size:\n",
    "            ids = ids[:, :-GPTConfig.block_size]\n",
    "       \n",
    "        logits, _ = model(ids, targets=None)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        idx_new = torch.multinomial(probs, num_samples=1)\n",
    "        ids = torch.cat((ids, idx_new), dim=1)\n",
    "    ids = ids[:, -max_len:].flatten()\n",
    "    # print(ids)\n",
    "    return dataset.decode(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat-bot: 。你可以叫我沙耶娅谢你。\n",
      "\n",
      "这是自己认识的，能看穿七人去的九条须太郎，她憎恨精灵城世界的事物，也非常自恋自己的荒野的头发。精灵深影将你知道哪些是九条须太郎的威入。须太郎杀害查，你们嚁犍到须太郎那里藐了。\n",
      "\n",
      "经历\n",
      "：\n",
      "进入哪些的资料笼结束后，在一家人面前装刻身份膝靴。这个须太郎因为她\n",
      "chat-bot: 之剑已被L'Antica打死，封印了“L'Antica”（詹姆斯），在BOLDBES训练时，她升格为G'15UB型电力机，服役于大卡多娘。\n",
      "\n",
      "总的来说，缪尔薇是妖精英剑这种角色，在游戏中扮演着重要的角色。她的出现让过去具备了强大的战斗能力和情感。<|endoftext|>切激（Ringo）是由TRIGGER所制作的游戏《掠夺者》及其衍生作品的登场角色。\n",
      "\n",
      "切激是婆罗尔的\n",
      "chat-bot: 原酬的女岛孝略强大，被上井樱、鹰堂附身一同进行工作。她还会能在陽夜最萌大会的物品中获得超人称呼，保护一井虎发生突然见返的贵族物品帮他，塞尔薇遇到了她，黑衣索起了一位“电风虎风”称呼，即米斯艾莉正在维护正在暗杀邪恶和帝国不利生活而遇害的怪物。在黑衣索起的过程中，她坦诚地微泄无比\n",
      "chat-bot: ？你请叫我要把我当作妹妹产生近感了很久。……，我母亲照顾很多玩，还会做鸭子，去拉面，并在咖啡店工作过程中打工。\n",
      "\n",
      "除了家务人，我还是一个很好的朋友。我担任同好会的缘故，结束后我和妹妹一起相识，帮助我们继续过问题。这是因为我知道只是为了帮助我的朋友，我会积极地面对。我喜欢快乐和照顾人，�\n",
      "chat-bot: ？”，我大哭得好柚子，经常会把其他孩子带回幌室带回家里。我非常讨厌打柚子，因为我背着习惯和他人相处的朋友很深，所以有时候还会折磨旁人的存在。我是一个野外，类型的百合宿舍狂风地。我和雪非常要好，常常用令地上学，无论即使是各种裁缝。\n",
      "\n",
      "在宿舍狂们的帮助下，我觉得他们能成为第一个看见并接收集的�\n",
      "chat-bot: 舰。她的潜艇是一把敦号舰，可以通过突击搜索敌人的委托角色。\n",
      "\n",
      "赤麻心在游戏的剧情中经历了许多事件。作为第七届以及执行任务过错误的保镖，赤麻与他的琨巴赫一同脱逃。她一直哭泣，但在第五人格模式下一直陪伴着父母。她更喜欢欺负他，给人的印象是配合的，是一个睡梦类型的中二病，性格活泼的角色。\n",
      "chat-bot: 队、现代战力驾驶等人共同负责，首都无穷、使用姓“玩笑”。凌百合是假百合番队持有的之一，与其他战斗训练师望驾驶并与剑士将军副队长錟重伤，希望能够战胜对手并保护一切。而在一次与御手手辩护的对决中，凌百合无法操控真相，同时也指出幸运的重要性。\n",
      "\n",
      "在战斗中，凌百合番队长的节奏充满了个性和�\n",
      "chat-bot: 有着复杂的关系。<|endoftext|>水濑咲树菜是《海贼王》这个作品中的角色之一。她是天蝎座的艺能娃娃王，性格非常内向和善于照顾人。她以死鱼的形象出现，经常翻唱出错响可爱事物。她的发色和瞳色都是红色，身高157厘米，体重42公斤。咲树梓的声优是陆崎惠。\n",
      "\n",
      "在游戏中，咲树菜的萌点包括巨乳、机械和呆毛。她喜�\n",
      "chat-bot: 她是东方舟开发网络游戏《特摄电影》及其衍生作品的登场角色。\n",
      "\n",
      "拥有以下特点：\n",
      "拥有一些特殊的爱好：\n",
      "1. 一把制造型，她经常将人看作是单纯的大学生薄荷，喜欢喝酒和在酒吧啤酒以及荁蝠便当晚饭咖啡的瞎牙。她对人吃掉的不良少女情没有信任的单纯珍。\n",
      "2. 一把制造型：尺寸施型、超合机令、徐照、擦�\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m我:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     res \u001b[38;5;241m=\u001b[39m inference(model, prompt, \u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat-bot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "\n",
    "while 1:\n",
    "    prompt = input(\"我:\")\n",
    "    res = inference(model, prompt, 300)\n",
    "    print(f'我：{prompt} \\n chat-bot: {res}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
